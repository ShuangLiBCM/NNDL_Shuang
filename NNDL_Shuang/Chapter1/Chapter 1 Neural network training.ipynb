{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Initialize neural network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize object\n",
    "class Network (object):    \n",
    "\n",
    "    def __init__(self,sizes):\n",
    "    # index of sizes represent layer number, each value represents num of neurons within each layer\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]      \n",
    "        # np.random.randn gives a standard normal distribution, enable the stochastic gradient descent\n",
    "        # each recipient neuron will need a bias term\n",
    "        # biases will have layer number -1 array, each array values for each neuron\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "        # Weights will be matrix determining by the number of input and output neurons of the network\n",
    "        # Zip combine the input elements with the same index as tuple\n",
    "        # e.g if a = [1,2,3], b = [3,4,5], then zip(a[:-1],b[1:]) will give [(1,4),(2,5)]\n",
    "\n",
    "# Realize a sample network\n",
    "net = Network([2,3,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quesiton:\n",
    "1. In python2, we could check the result of zip, how to do it in python3?\n",
    "Answer: use list to force it!\n",
    "2. More useful ways of zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### define sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the feedforward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def feedforward(self,a):\n",
    "        # Return the output of the network if \"a\" is provided\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            a = sigmoid(np.dot(w,a)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement the mini-batch stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def SGD(self, training_data,epochs,mini_batch_size,eta,test_data = None):\n",
    "    # training data is a list of tuples \"(x,y)\" represnting the training input and the desired outputs.\n",
    "    # If \"test_data\" is provided, the network will be evaluated against the test data after each epoch, and priting partial result \n",
    "    # eta is the learning rate\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        for j in xrange(epochs):\n",
    "            np.random.shuffle(training_data)  # generate the random shuffled training_data\n",
    "            mini_batchs = [\n",
    "                training_data[k:k+mini_batch_size] for k in xrange(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch,eta)\n",
    "            if test_data:\n",
    "            # if test_data is provided, the network will be evaluated after each epoch of training\n",
    "                print (\"Epoch{0}:{1}/{2}\".format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print (\"Epoch{0} complete\".format(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: \n",
    "1. Why use\n",
    "            mini_batchs = [\n",
    "                training_data[k:k+mini_batch_size] for k in xrange(0,n,mini_batch_size)]\n",
    "   instead of \n",
    "                training_data[k:k+mini_batch_size] for k in xrange(0,n-mini_batch_size-1,1)]\n",
    "                \n",
    "   Answer: not wasting the batches for every epoch, as little overlap as possible\n",
    "   \n",
    "2. How to correctly understand \n",
    "            for mini_batch in minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Single epoch of training with mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Update the network's weights and biases by applying gradient descent using backpropagation to a single mini_batch.\n",
    "    # The \"mini_batch\" is a list of tuples \"(x,y)\", and the \"eta\" is the learning rate\n",
    "\n",
    "    \n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # for array generate by np, np.array.shape gives the size infor\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in mini_batch:\n",
    "        # for a single epoch, every training pair in mini_batch contributes to the training\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb,dnb in zip(nabla_b,delta_babla_b)]\n",
    "            nable_w = [nw+dnw for nw,dnw in zip(nabla_w,delta_babla_w)]\n",
    "        # dnb and dnw has been summed for len(mini_batch) times, normlization needed\n",
    "        self.weights = [w - (eta*nw)/len(mini_batch) for w,nw in zip(self.weights,nabla_w)]\n",
    "        self.biases = [b - (eta*nb)/len(mini_batch) for b,nb in zip(self.biases,nabla_b)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation for fast calculation of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def backprop(self,x,y):\n",
    "        # return a tuple (nabla_b,nabla_w) representing the gradient for the cost function C_x.\n",
    "        # nabla_w and nabla_b are layer by layer lists of numpy arrays, similar to self.biases and self.weights\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activation = [x]  # list to store all the activaitons, layer by layer\n",
    "        zs = [] # list to store all the z vectos, layer by layer\n",
    "        \n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            z = np.dot(w,activation)+b\n",
    "            zs.append(z)\n",
    "            activaiton = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1],y)*sigmid_prime(zs[-1])\n",
    "        nabla_b [-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta,activation[-2].tranpose())\n",
    "        for l in xrange(2,self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose().delta)*sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta,activations[-l-1].transoise())\n",
    "        return (nabla_b,nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "1. Version conflict for np.array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def evaluate(self,test_data):\n",
    "    # given the current weights and biases, evaluate the correction rate\n",
    "    # reture the numebr of test inputs for which the neual network outputs the corret result. \n",
    "    # The neural network's output is assumed to be the index of whichever neuron in the final layer has the highes activation\n",
    "    \n",
    "        test_results = [(np.argmax(self.feedforward(x)),y) for (x,y) in test_data]\n",
    "        return sum(int(x==y) for (x,y) in test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost derivative function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def cost_derivative(self,output_activations,y):\n",
    "    # return vector of partial derivatives partial C_x/partial a for the output activation\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)/(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Object-oriented programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Enable function targets a certain object\n",
    "Group objects with functions\n",
    "Temlate-like, shared across different objects\n",
    "\n",
    "From the extracted commonallity and make them into the template\n",
    "\n",
    "Start with a constructor, generate the template\n",
    "\n",
    "__init__(self,input)\n",
    "    initialze all the variables\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

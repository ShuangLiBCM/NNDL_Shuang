{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron was developed by Frank Rosenblatt, and it's one of the earliest kinds of *neural network*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the *neurons* model we'll encounter is *sigmoid neuron*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron unit has multiple inputs $x_1, x_2, \\ldots,$ and produces a single binary output. How is the binary output computed from the inputs? Each of the input is weighted by respective weights $w_1, w_2, \\ldots,$, and the neuron's output (0 or 1) is determiend by comparing the weighted sum $\\sum_j{w_j x_j}$ against a *threshold* $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressed more concisely,\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{output}= \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } \\sum_j{w_j x_j} \\leq \\theta\\\\\n",
    "    1, & \\text{if } \\sum_j{w_j x_j} \\gt \\theta\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above expression can be further simplified by evaluating $\\sum_j{w_j x_j} = w \\cdot x$ and also reexpressing the inequality as that involving $w\\cdot x - \\theta = w\\cdot x + b$ where $b = -\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{output}= \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } w\\cdot x + b \\leq 0\\\\\n",
    "    1, & \\text{if } w\\cdot x + b \\gt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $b$ is the *bias* and can be thought of as the propensity for perceptron unit to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: NAND gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron with two inputs $x_1$ and $x_2$ can be made to act as a *NAND* gate by setting both $w_1=w_2=-2$ and $b=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you can see that the output is $1$ (*true*), for $(x_1, x_2)$ values of $(0, 0)$, $(1, 0)$, and $(0, 1)$, but *not* for $(1, 1)$! This is precisely the behavior of a NAND logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact a perceptron unit can be turned into a NAND gate suggests that with enough such neuronal units, any logic circuit can be realized in perceptrons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought the fact that perceptrons can perform everything a computer can do is a pretty cool finding in its own right, it still doesn't suggest exactly how we can build a network that achieves a desired computation. Ideally, we would like for the perceptrons to *learn*, by correctly adjusting the weights $w$ and bias $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make learning tractable, we would like it such that a *small change* in parameters (i.e. $w$ and $b$) leads to *small change* in the output.\n",
    "\n",
    "Only problem is that perceptrons as we have it now, does *not* have such a property! In fact, a small changes in the weights $w$ or bias $b$ can cause the output of the perceptrons to flip from 0 to 1 in the second layer, which can subsequently lead to flips and flops throughout the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such drastic changes resulting from a small changes in the parameter makes learning very problematic. However, this problem can be mitigated by employing a slightly different kind of neuron called *sigmoid* neuron. In particular, sigmoid neurons have the desired feature of small changes in parameters leading to small changes in the output. Let's see how that works out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a perceptron, a sigmoid neuron has inputs $x_1, x_2, \\ldots$. However, instead of only takeing a binary value of 0 or 1, the input (and as you'll see, the output) can take on any values between 0 and 1. Just like in the case of a perceptron, sigmoid neurons have weights $w_1, w_2, \\ldots$ that multiply onto the inputs and a bias term $b$. However, the output is not binary value obtained by thresholding $w\\cdot x + b$. Rather, the quantity $w\\cdot x + b$ is run through the *sigmoid function* $\\sigma(z)$ defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all togeter, the output of a sigmoid neuron with inputs $x_1, x_2, \\ldots$ with weights $w_1, w_2, \\ldots$ and bias $b$ is:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\frac{1}{1 + \\exp(-\\sum_j{w_j x_j} - b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of sigmoid function, let's plot it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1, 1.1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEOCAYAAACJlmBtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBBERAZQQREQkRglBREQA+P8j8yPUo0WDSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1103265f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = linspace(-10, 10, 100)\n",
    "output = 1 / (1 + exp(-z))\n",
    "perceptron = z > 0\n",
    "plot(z, output, label='sigmoid')\n",
    "plot(z, perceptron, label='perceptron')\n",
    "legend(loc='best')\n",
    "xlabel('$z = w \\cdot x + b$', fontsize=14)\n",
    "ylabel('output')\n",
    "ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the x-axis is the value of the sum of weighted input and the bias ($z = w \\cdot x + b$). The expected output of the perceptron (green) highlights the *jumpy* nature of the perceptron response, with a sharp boundary at $z=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid neuron (blue), on the other hand, has a much more smooth and gradual transition between 0 and 1. From this it can be seen that sigmoid neuron is essentially a smoothened out version of the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have guessed by my mention of *small changes*, the smoothness in the sigmoid neuron's output renders the function *differentiable*. This means that with a small change in the weights $\\Delta w$ and bias $\\Delta b$, there is a small change in the output $\\Delta y$ that can be approximated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta y \\approx \\sum_j\\frac{\\partial y}{\\partial w_j}\\Delta w_j + \\frac{\\partial y}{\\partial b}\\Delta b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the smoothness (differentialibility) of the sigmoid function $\\sigma(z)$ allows us to approximate the change in output, $\\Delta y$ as (locally) linear functions of the changes in parameters, $\\Delta w$ and $\\Delta b$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's introduce some terminology in to the game. The sigmoid function $\\sigma(z) = \\sigma(w \\cdot x + b)$ is *an example* of an *activation function* $f(\\cdot)$. We can generalize both the perceptrons and sigmoid neurons as a network where the output $y = f(z) = f(w \\cdot x + b)$ for some choice of activation function $f(\\cdot)$. Obviously, choosing $f(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$ renders the neuron a *sigmoid neuron*. Alternatively, choosing $f(\\cdot)$ to be a step function,\n",
    "\n",
    "$$\n",
    "f(z)= \n",
    "\\begin{cases}\n",
    "    0, & z \\leq 0\\\\\n",
    "    1, & z \\gt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "renders the neuron a *perceptron*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of activation function $f(\\cdot)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, as long as $f(\\cdot)$ is smooth and differentiable, the locally linear relationship between changes in the parameters ($w$ and $b$) and the output ($y$) holds true. Then why use sigmoid function? This has a lot to do with the differentiated form of the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{d\\sigma(z)}{dz} &= \\frac{d}{dz}\\frac{1}{1 + e^-z} \\\\\n",
    "&= \\frac{e^{-z}}{(1+e^{-z})^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now note that:\n",
    "\n",
    "$$\n",
    "1 - \\sigma(z) = 1 - \\frac{1}{1 + e^{-z}} = \\frac{e^{-z}}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, above derivative can be reexpressed as follows:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(z)}{dz} = \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}} = \\sigma(z) (1 - \\sigma(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll come back to this when we start using these derivatives of activation function to optimize the parameters for the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notable differnce of a sigmoid neuron from a perceptron is that the output is no longer binary, but rather takes on any value between 0 and 1. How shoudl we interpret it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the output of such *continout* neuron unit really depends on context. If the output is supposed to be a pixel intensity, then the continous value can be mapped out to the grayscale on the pixel. However, if the output *is* supposed to be a binary (e.g. presence or absence of a face in the image), then we can appply thresholding on the output variable (e.g. at 0.5) to *convert* the output into a binary value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication of $w$ and $b$ by a positive constant in perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a perceptron with weights $w$ and bias $b$, multiplying both of these by a positive constant $c$ does *not* alter the perceptron's behavior (output) in response to any input. This can easily be seen by comparing the behavior of the original perceptron against a perceptron with all weights and bias multiplied by $c$ such that $\\hat{w} = cw$ and $\\hat{b} = cb$. Given some arbitrary input $x$, the original perceptron's output $y$ is determined as in :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "0, & \\sum_j{w_j x_j} + b \\leq 0 \\\\\n",
    "1, & \\sum_j{w_j x_j} + b \\gt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the output of the modified perceptron $\\hat{y}$ to the same input can be determined as in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "0, & \\sum_j{c w_j x_j} + c b \\leq 0 \\\\\n",
    "1, & \\sum_j{c w_j x_j} + c b \\gt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be rewritten as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "0, & c(\\sum_j{w_j x_j} +  b) \\leq 0 \\\\\n",
    "1, & c(\\sum_j{w_j x_j} +  b) \\gt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But notice that, given $c > 0$,\n",
    "\n",
    "$$\n",
    "c\\left(\\sum_j{w_j x_j} + b\\right) \\leq 0 \\iff \\sum_j{w_j x_j} + b \\leq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and therefore,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "c\\left(\\sum_j{w_j x_j} + b\\right) \\gt 0 \\iff \\sum_j{w_j x_j} + b \\gt 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the output of original and the modified perceptron will be the same for any input. provided that the input to the first layer remains the same, the all outputs of the modified perceptrons on the first layer don't change from scaliing with $c > 0$. Since the output of the first layer is the input to the second and it again is unchanged, it can be seen (by induction), that multiplying all parameters of a perceptron by a positive constant do *not* alter the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication of $w$ and $b$ by a positive constant in sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided that all sigmoid neurons have non-zero $w \\cdot x + b$ for a particular input $x$, the output of a sigmoid neuron is:\n",
    "\n",
    "$$\n",
    "y = \\sigma(w \\cdot x + b) = \\frac{1}{1 + \\exp(-w \\cdot x - b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we scale both $w$ and $b$ by a positive constant, $c > 0$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\sigma(cw \\cdot x + cb) = \\frac{1}{1 + \\exp(-c(w \\cdot x + b))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the value of $c$: $c \\to \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{c \\to \\infty} \\hat{y} &= \\lim_{c \\to \\infty} \\frac{1}{1 + \\exp(c(w \\cdot x + b))} \\\\\n",
    "&=\n",
    "\\begin{cases}\n",
    "0, & \\text{if } w \\cdot x + b \\lt 0 \\\\\n",
    "1, & \\text{if } w \\cdot x + b \\gt 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it can be seen that the output $\\hat{y}$ approaches a step function based on the value of $w \\cdot x + b$ and is exactly equal to a perceptron!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if $wx + b = 0$? From above expressions, it can be seen that,\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(c \\cdot 0) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "and it does not depend on the value of $c$ at all. Hence, if some of the sigmoid neurons equals to $0$ for a value of $x$, that perceptron will always yield 0.5 regardless of the value of scaling factor $c$ and thus does *not* approach the behavior of a perceptron which should be 0 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Architecture of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When drawing and describing neural networks, it is helpful to know certain terminologies to describe parts of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In drawing neural network, the network is typically arranged in such way that information flows from *left to right*. Therefore, the left end corresponds to the input into the network, whereas the right end corresponds to the output of the network. Under this configuration, the left most layer of neurons are referred to as *input neurons*. Correspondingly, the right most layer of neurons are the *output neurons*.\n",
    "The middle layers (those that are nither input nor output) are called *hidden layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusingly, such multilayer neural networks may be referred to as multilayer perceptrons (MLP) despite the fact that the network is not composed of perceptrons but rather with sigmoid neurons. This reference exists for largely historical reasons, and it is best avoided (although should be aware of it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All networks that will be covered in this book are *feedforward* networks - information only flows forward and never backward. Graphically speaking, this means there cannot be any loops in the network graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists a class of neural networks where such loops are allowed and these are known as *recurrent neural networks*. In such networks, the trick is to separate the activation of units in time, such that there is a delay between an input layer firing and the subsequent layer collecting and responding to the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit classification - simple network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the basic network lingos covered, it's time to *build* a network that does something useful, and we'll start by tacking the famous handwritten digit classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem can be divided into two parts. First is the *image segmentation*, where an image of multiple digits (e.g. 8432) needs to be split into smaller images each containing only one digit (e.g. 8, 4, 3, and 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is, of course, the *classification* of the digits into one of the ten possible categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem of turning on the binary digit in response to the readout, we have to consider when does each digit need to be turned on. To see this, let's write out the 4-bit binary representation of the numbers 0-9:\n",
    "\n",
    "* 0: 0000\n",
    "* 1: 0001\n",
    "* 2: 0010\n",
    "* 3: 0011\n",
    "* 4: 0100\n",
    "* 5: 0101\n",
    "* 6: 0110\n",
    "* 7: 0111\n",
    "* 8: 1000\n",
    "* 9: 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reorganize when each bit is turned on.\n",
    "\n",
    "* bit 1 (least significant bit): 1, 3, 5, 7, and 9\n",
    "* bit 2: 2, 3, 6, and 7\n",
    "* bit 3: 4, 5, 6, and 7\n",
    "* bit 4 (most significant bit): 8 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight for each unit should look something like this:\n",
    "\n",
    "* bit 1: w = k [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "* bit 2: w = k [0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
    "* bit 3: w = k [0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
    "* bit 4: w = k [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "\n",
    "with b = -0.5*k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where k is some positive constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use matrix computation to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could solve for transfer matrix X and bias b, which give rise to \n",
    "aX + b = c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a is an 1 by 10 array given by activity from the old output layer. c is an 1 by 4 array representing activity of the new output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if we need all the 0 - 9 values being correctly transfered to the binary output thorugh X and b, they will need to satisify AX + b =C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A is an 10 by 10 matrix indicating activity from the old output layer with 0 - 9 as the correct digits. Given the definition of the network, it's easy to see that A is an 10 by 10 identity matrix, i.e. I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is the response of the new output layer with 0 - 9 as the correct digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =\n",
      "[[0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 1 0]\n",
      " [0 1 1 1]\n",
      " [1 0 0 0]\n",
      " [1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.matrix('0 0 0 0;0 0 0 1;0 0 1 0;0 0 1 1;0 1 0 0;0 1 0 1;0 1 1 0;0 1 1 1;1 0 0 0;1 0 0 1')\n",
    "print ('C =')\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that IX+b = X+b = C, X = C - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll quantify how well our neural network model is able to classify images of digits using a cost function:\n",
    "\n",
    "$$\n",
    "C(w, b) = \\frac{1}{2n} \\sum_x \\lVert y(x) - a\\rVert \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where, $w$ is the collection of *all* weights for our neurons, and $b$ is the collection of *all* bias terms. $n$ is the total number of training inputs, and $a$ is the vector of outputs when $x$ is the input. $y(x)$ is the function that our network is trying to apprximate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function $C$ above will be referred to as the *quadratic* cost function. It's also commonly referred to as the *mean squared error* or *MSE*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the cost function $C$, it's obvious that $C \\geq 0$ for all $x$, and that $C \\to 0$ only as $a \\to y(x)$ for all $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, the goal of *training* of our network is to minimize the above cost function, as a function of weights $w$ and biases $b$. To achieve this, we are going to use optimization algorightm known as *gradient descent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why quadratic cost function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering why aren't we simply counting the number of images incorrectly classified rather than looking at a surrogate measure of the quadratic cost function. The problem with using the number of misses is that it's not a smooth function of the parameters $w$ and $b$. Using the number of misses, there will be many values of $w$ and $b$ for which small changes in $w$ and/or $b$ do *not* result in any difference in the cost, making it difficult for an algorithm to know how to modify the parameters to improve the performance (decrease the cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we use a smooth cost function such as the quadratic cost function, any small changes in the parameters will give rise to a small but noticeable difference in the cost, allowing for the algorithm to infer how to modify the parameter to improve the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing a 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case of minimizing a generic scalar output function first. We'll denote this function as $C(v)$ where $v=v_1, v_2, \\ldots$. We'll start by considering the case of 2D input: $v = (v_1, v_2)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such a function, it might be tempting to try to solve the problem analytically - by finding values of parameters for which partial derivative $\\frac{\\partial C}{\\partial v_i} = 0$. Although this may work for a simple function, such method doesn't scale well with increasing number of input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do then? Turns out we can use our intuition about a ball rolling down hill to arrive at a method of fidning a minimum! We will randomly choose a starting point $(v_1^{(0)}, v_2^{(0)})^T$, and will *simulate* the motion of a hypothetical ball positioned there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essense, we want to move the ball such that it moves downward - that is to a position with a lower value of $C(v)$. To achieve this, let's use Calculus local linear approximation to the function:\n",
    "\n",
    "$$\n",
    "\\Delta C \\approx  \\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2}\\Delta v_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\frac{\\partial C}{\\partial v_1}$ and $\\frac{\\partial C}{\\partial v_2}$ are evaluated at the current location $(v_1^{(0)}, v_2^{(0)})^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation can be simplified by vectorizing everything. Namely,\n",
    "\n",
    "$$\n",
    "\\Delta v = (\\Delta v_1, \\Delta v_2)^T\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla C = \\left(\\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2}\\right)^T\n",
    "$$\n",
    "\n",
    "where $\\nabla C$ is of course the *gradient* of $C$ evaluated at current location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the above expression can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\Delta C \\approx \\nabla C \\cdot \\Delta v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to point $\\Delta v$ in the right direction so as to attain smallest (most negative) value of $\\Delta C$, then we should choose:\n",
    "\n",
    "$$\n",
    "\\Delta v = -\\eta \\nabla C\n",
    "$$\n",
    "\n",
    "where $\\eta$ is some small, positive parameter (known as the *learning rate*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this choice of $\\Delta v$, we can see that\n",
    "\n",
    "$$\n",
    "\\Delta C \\approx -\\eta \\nabla C \\cdot \\nabla C = -\\eta \\lVert \\nabla C \\rVert ^2,\n",
    "$$\n",
    "\n",
    "guaranteeing $\\Delta C$ to be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this *small* movement of the ball $\\Delta v$ that can decreases the value of $C$, we can compute the ball's next position as:\n",
    "\n",
    "$$\n",
    "v^{(i+1)} = v^{(i)} + \\Delta v = v^{(i)} - \\eta \\nabla C\n",
    "$$\n",
    "\n",
    "and this is called an *update rule* for $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order for the above update function to work and keep yielding a new position for which $C(v^{i+1)}) \\leq C(v^{i})$, the learning rate $\\eta$ must be small enough to make the local linear approximation a good one. However, we also don't want to pick too small of a value of $\\eta$ as that could redner the learning algorithm very slow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above *derivation* of gradient descent update rule can be easiy extneded to the case of a function with more than two variables. Suppose $C$ is a function of $m$ variables, $v_1, v_2, \\ldots, v_m$. The change in $C$, $\\Delta C$, for a *small change* $\\Delta v = (\\Delta v_1, \\Delta v_2, \\ldots, \\Delta v_m)^T$ is:\n",
    "\n",
    "$$\n",
    "\\Delta C = \\nabla C \\cdot \\Delta v\n",
    "$$\n",
    "\n",
    "just as before, where\n",
    "\n",
    "$$\n",
    "\\nabla C = \\left( \\frac{\\partial C}{\\partial v_1}, \\ldots, \\frac{\\partial C}{\\partial v_m} \\right)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then choose\n",
    "\n",
    "$$\n",
    "\\Delta v = -\\eta \\nabla C\n",
    "$$\n",
    "\n",
    "and guarantee that (for small $\\eta$), $\\Delta C$ will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, again, we can *update* the location as\n",
    "\n",
    "$$\n",
    "v^{(i+1)} = v^{(i)} + \\Delta v = v^{(i)} - \\eta \\nabla C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show that the choice of $\\Delta v$ is in a sense the optimal choice.\n",
    "\n",
    "Based on Cauchy-Schwartz inequality\n",
    "$$\n",
    "\\lvert \\nabla C \\cdot \\Delta v \\rvert \\leq \\lVert \\nabla C \\rVert \\lVert \\Delta v \\rVert\n",
    "$$\n",
    "\n",
    "Hence the smallest possible value of $ \\Delta C \\approx \\nabla C \\cdot \\Delta v$ is $-\\lVert \\nabla C \\rVert \\lVert \\Delta v \\rVert$, and this occurs when $\\Delta v \\propto -\\nabla C$.\n",
    "\n",
    "Since $\\lVert \\Delta v \\rVert = \\epsilon$, we can choose $v = -\\eta \\nabla C$ where $\\eta = \\frac{\\epsilon}{\\lVert \\nabla C \\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When the function $C$ is a function of a single variable, then the method of gradient descent can be thought of as going down the slope on a 2D valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1113df518>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEACAYAAACeQuziAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf5NyKC9es2gwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113df4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = linspace(-3, 3)\n",
    "plot(x, x**2)\n",
    "axis('off')\n",
    "annotate('', xy=(1.5, 2), xytext=(2, 4),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "plot(2, 4, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent applied to neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with an understanding of gradient descent method, let's see how this can be used to optimize our cosft function $C(w, b)$ with respect to the weights $w$ and biases $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basically, our \"location\" in the search space will be defined by the combined vector $(w_1, \\ldots, w_k,\\ldots, b_1, \\ldots, b_l, \\ldots)^T$, with a corresponding $\\nabla C$ with components $\\frac{\\partial C}{\\partial w_k}$ and $\\frac{\\partial C}{\\partial b_l}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for each component of weights and biases will be:\n",
    "\n",
    "$$\n",
    "w_k^{(i+1)} = w_k^{(i)} - \\eta \\frac{\\partial C}{\\partial w_k}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b_l^{(i+1)} = b_l^{(i)} - \\eta \\frac{\\partial C}{\\partial b_l}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above update rules will allows us to look for a minimum in the const function $C$, the evaluation of the gradient of $C$ may be costly. To see this, recall that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{align}\n",
    "C(w, b) &= \\frac{1}{n}\\sum_x{C_x} \\\\\n",
    "&= \\frac{1}{n}\\sum_x{\\frac{\\lVert y(x) - a \\rVert^2}{2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the evaluation of the above cost function requires the evaluation of $C_x$ for each of the $n$ samples ($x$). Hence the computational load for evaluating $C$ as well as $\\nabla C$ scales linearly with the number of training samples $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this can be hindering as the number of samples increases because each cycle of update step may take a really long time! (You also have to consider the fact that the shape of the cost function *can* get more complex with more samples (as you are effectively mixing more functions!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we deal with this? One potential method is to *approximate* the $C(w, b)$ by *sampling* $m$ sample training points out of $n$ .\n",
    "\n",
    "$$\n",
    "\\frac{1}{m}\\sum_{j=1}^{m}{\\nabla C_{x_j}} \\approx \\frac{1}{n}\\sum_x {\\nabla C_x} = \\nabla C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approximation tends to be better the more samples $m$ we use out of total of $n$ training data: that is it gets better as $m \\to n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, to speed up our learning process when we have large number of training points (i.e. large $n$). This means that we can use this approximation in the update step, such that:\n",
    "\n",
    "The update rule for each component of weights and biases will be:\n",
    "\n",
    "$$\n",
    "w_k^{(i+1)} = w_k^{(i)} - \\frac{\\eta}{m} \\sum_{j=1}^{m} \\frac{\\partial C_{x_j}}{\\partial w_k}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b_l^{(i+1)} = b_l^{(i)} - \\frac{\\eta}{m} \\sum_{j=1}^{m} \\frac{\\partial C_{x_j}}{\\partial b_l}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now it would be rather wasteful to use only $m$ out of $n$ samples you have. Can you make good use of the remaining $n-m$ samples? \n",
    "\n",
    "The trick here to use a *different set of $m$ samples* on each run of the above update algorithm, and you draw the set of $m$ samples from the remaining samples randomly. Hence, you complete the update on *mini-batches* of $m$ samples at a time, and repeat until you exhaust all $n$ samples, which is said to have *completed an epoch*. Once you complete an epoch (exhausted $n$ samples), you start over, repeating the process until training update converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above described algorithm for *approximating* a full gradient descent procedure is known as *stochastic* gradient descent with mini-batch size of $m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing gradient descent with $n$ samples, notice that the term $\\eta$ is natually scaled by a factor $\\frac{1}{n}$ as shown below:\n",
    "\n",
    "$$\n",
    "w_k^{(i+1)} = w_k^{(i)} - \\eta \\frac{\\partial C}{\\partial w_k} = w_k^{(i)} - \\frac{\\eta}{n} \\sum_{j=i}^n {\\frac{\\partial C_{x_j}}{\\partial w_k}}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b_l^{(i+1)} = b_l^{(i)} - \\eta \\frac{\\partial C}{\\partial b_l} =  b_l^{(i)} - \\frac{\\eta}{n} \\sum_{j=1}^n{\\frac{\\partial C_{x_j}}{\\partial b_l}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling by number of samples $\\frac{1}{n}$ is sometimes ommited, thus simply summing over invidual sample gradients rather than averaging. This is useful when the number of training samples is not known in advance (e.g. samples are getting generated dynamically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is compared to a political poll - *sampling*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extreme version of stochastic gradient descent is to use mini-batch size of 1 ($m=1$)! That means, we update the parameters $w$ and $b$ by computing the gradient on one sample at a time. This procedure is known as *online* or *incremental* learning, and is often compared to *one-shot learning* in human beings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big advantage of this is the fact that the learning can take place *dynamically* as more samples are made available - when a new sample arise, we can simply update the parameter! Also of course the speed of computation is a benefit. You cannot get much simpler than computing just one gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using $m=1$ can cause the gradient to deviate largely from what the gradient would be for all samples together. This means that now there is more chance that an anomaly sample to (negatively) affect the learning compared to using larger mini-batch size. For example, if a training sample is *mislabeled*, it can have a strong effect on the network parameters, much more so than if it was just a sample out of $m$ samples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing high dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author has a very good but all too often understated point in the dicussuion of working with high dimensional data. As the author points out, no one can *visualize* data in 4 dimensions or beyond (if you include color, perhaps 4D is actually *visualizable* but such method doesn't work for any higher dimension). The trick is to *extrapolate* intuition from lower dimension variants of the data. It is also common to see how something changes when transitioning from using 2D data to 3D data set, and to use this as the basis for imagining what happens in 4D and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to conclude Part 1 of the Chatper 1 at this point, as we have visited all concepts needed to now actually implement a neural network and train it on real data sets. In the next part, I'll cover the rest of the Chapter 1 where we actually tackle the problem of *implementing* and *training* a neural network to perform hand-written digit calssification, using everything we learned thus far!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

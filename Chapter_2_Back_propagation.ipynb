{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote each weight and bias by the location of the neuron (layer) and index as\n",
    "$\n",
    "w^l_{jk}\n",
    "$,\n",
    "$\n",
    "b^l_{j}\n",
    "$\n",
    ", the actvity of neuron $j$ in layer $l$ will be \n",
    "\n",
    "$ \n",
    "a^l_j = \\delta(\\sum w^l_{jk}a^{l-1}_k+b^l_j)\n",
    "$\n",
    "\n",
    "If we denote the intermedaite variable\n",
    "$\n",
    "z^l_j = \\sum w^l_{jk}a^{l-1}_k+b^l_j\n",
    "$\n",
    "then,\n",
    "$\n",
    "a^l_j = \\sigma(z^l_j)\n",
    "$\n",
    "$z^l_j$ is the weigted sum of input variables. $a^l_j$ is the output variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We could rewrite the equation in a vector format\n",
    "$\n",
    "a^l = \\delta(w^la^{l-1}+b^l)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two assumptions we need about cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a revision about what the format of cost function might be, here is the cost function we used in last chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "C = \\sum_x\\frac{1}{2m}||y(x)-a^L(x)||^2\n",
    "$\n",
    ",where x is single training example and y(x) is the corresponding labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions we need on cost functions:\n",
    "1. Cost functins shoud be able to be written as sum over all the training examples.\n",
    "\n",
    "Because what the backpropagation algorithm letting us do is to compute the partial derivatives $\n",
    "\\frac{\\partial C_x}{\\partial w}$,$\\frac{\\partial C_x}{\\partial b}$ for each sample x and then recover $\n",
    "\\frac{\\partial C}{\\partial w}$,$\\frac{\\partial C}{\\partial b}$ by averaging across them.\n",
    "\n",
    "2. The cost function should be a function on the output of the network.\n",
    "\n",
    "Think about the selection of variable versus parameter of a function. A variable should be the factor that controls the cost and should be changable. A parameter, however, is fixed and could only influence the exact value of the cost function.Therefore, the cost function is over $x$ and network output $a^l_j$. $y(x)$, which is determined by $x$, is a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Hadamard product, $s \\odot t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hadamard product indicate the elementwise multiplication\n",
    "$(s\\odot t)_j = s_j * t_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is about how change of weigths and biases change the performance of the cost function. \n",
    "The pertubation of either weights or biases lead to change of $z^l_j$ by \n",
    "$\n",
    "\\Delta z^l_j\n",
    "$\n",
    "and leads to the final change of cost function by $\\frac{\\delta C^l_j}{\\delta z^l_j} \\Delta z^l_j $.\n",
    "If the cost function is sensitive to the perturbation, i.e., $\\delta C^l_j/\\delta z^l_j$ is big, we could adjust the weights and/or biases to reduce the cost function. Alternatively, if $\\delta C^l_j/\\delta z^l_j$ is close to zero, we should then spend little time on studying those weights and/or biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We called the sensitivity of the cost function over a specific neuron's output the error of step at the neuron $j$ of layer $l$\n",
    "\n",
    "### $\\delta^l_j \\equiv \\delta C^l_j/\\delta z^l_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the vectorization of the denotation, $\\delta^l$ is the error of layer $l$. Backpropagation will enable us to calculate the $\\delta^l$ of every layer and assign them to, $\\frac{\\delta C}{\\delta w^l_{jk}}$, $\\frac{\\delta C}{\\delta b^l_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question now becomes why we use the partial derivative over the weighted sum of the inputs $z^l_{j}$ instaed of the output $a^l_{j}$ as the measure of error. In fact, using either way will give rise to the same result. Given the algebraical simplification, the author chose to use $\n",
    "\\delta^l_j \\equiv \\delta C^l_j/\\delta z^l_j\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The four fundamental equations for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation 1: Error in the output layer L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j) $, -- (BP1)\n",
    "\n",
    "$ \\sigma'(z^L_j)$ indicates the slope of change of activation function $\\sigma$ at point $z^L_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we want the matrix-based form for backpropagation, we could rewrite the BP1 equation as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\delta^L  = \\nabla_aC \\odot \\sigma'(z^L) $ -- (BP1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the quadratic cost function $\\nabla_aC = (a^L - y)$, the full format of the matrix function will be \n",
    "\n",
    "$\\delta^L  = (a^L - y) \\odot \\sigma'(z^L) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also see from (BP1) that the error is dependent on the learning rate of the activation function. If we use sigmoid function as $\\delta$, when value of $z^L_j$ is too low or too high, $\\delta'(z^L_j)$ will be close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation 2: Error $ \\delta^l $ expressed with respect to the error at the next layer $ \\delta^{l+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l) $ -- (BP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinin (BP1) and (BP2), we could compute the error of all the layer. Start from BP1 for the output layer and then propagate back to the early layers.\n",
    "Similar as the saturating effect that influence the output layer, effect of $\\delta'(z^l)$ will lead to the lack of learning activity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation 3: Rate of change of the cost with respect to any bias in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\frac {\\partial C}{\\partial b^l_j} = \\delta^l_j $  -- (BP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the sensitivity of the cost function on the bias of the neuron equals to the error of at the neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equaiton 4: Rate of change of the cost with respect to any weight in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\frac {\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k\\delta^l_j $ -- (BP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity of the cost function of the weights of each neuron equals to the product of the input that pass on each weight and the error at the neuron.Another fact we could observe from (BP4) is that if $a^{l-1}_k$ is small, the gradient will descent slowerly. I.e., \n",
    "#### The weights output from low activity neurons will learn slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summerize the four key equations for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Error at the output layer\n",
    "### $\\delta^L_j = \\frac{\\partial C}{\\partial a_j^{L}}\\odot\\sigma'(z^L_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Error at the layer l given error at layer l+1\n",
    "### $\\delta^{l}_j = (w^{l+1})^T\\delta^{l+1}\\odot \\sigma'(z^l) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Influence of bias on cost function\n",
    "### $ \\frac{\\partial C}{\\partial b^l_{j}} = \\delta^l_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Influence of weights on cost function\n",
    "### $ \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k\\delta^l_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Prove for the 4 equations of backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the definition of error\n",
    "$\\delta^l_j \\equiv \\delta C^l_j/\\delta z^l_j$\n",
    "and the chain rule for partial derivatives, we could rewrite the error at the output layer unit\n",
    "\n",
    "### $\\delta^L_j = \\frac{\\delta C}{\\delta a^L_j}\\frac{\\delta a^L_j}{\\delta z^L_j}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $a^l_j = \\sigma(z^l_j), \\frac{\\delta a^L_j}{\\delta z^L_j} = \\sigma'(z^L_j)$, we will then obtain\n",
    "$\\delta^L_j = \\frac{\\delta C}{\\delta a^L_j}\\sigma'(z^L_j)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP2\n",
    "\n",
    "$\\delta^l = (w^{l+1})^T\\delta^{l+1}\\odot \\sigma'(z^l)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the definition\n",
    "$\\delta^l = \\frac{\\partial C}{\\partial z^l}$\n",
    "If we hope to express $\\delta^l$ through $\\delta^{l+1}$ we could introduced $z^{l+1}$\n",
    "Therefore, we have \n",
    "$\\delta^l = \\sum_k\\frac{\\partial C}{\\partial z^{l+1}_k}\\frac{\\partial z^{l+1}_k}{\\partial z^l}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
